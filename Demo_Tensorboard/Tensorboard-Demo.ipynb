{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tersor....Board....ing "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Watching the [introduction video](https://www.youtube.com/watch?v=eBbEDRsCmv4&t=1105s) [Tutorial](https://github.com/decentralion/tf-dev-summit-tensorboard-tutorial)\n",
    "\n",
    "If (Tensorboard == useful): \n",
    "    Install tensorflow https://www.tensorflow.org/install   \n",
    "else \n",
    "    Close this notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### start the tensorboard : \n",
    "\n",
    "> /Prometheus/src$ tensorboard --logdir=./logs_ann/\n",
    "> \n",
    "> TensorBoard 1.8.0 at __http://*****:6006__ (Press CTRL+C to quit)\n",
    "> \n",
    "\n",
    "Tips: if you open the 6006 url link but find nothing, calm down, double check the path. Wrong folder will give you empty board \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Wrong code could too, so the code begins here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL']='2'\n",
    "\n",
    "from os import listdir\n",
    "from tensorflow.python.client import timeline\n",
    "\n",
    "import calendar\n",
    "import time\n",
    "import arrow\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.python.keras.models import Sequential\n",
    "from tensorflow.python.keras.layers import Dense, GRU,Input\n",
    "from tensorflow.python.keras.optimizers import *\n",
    "from tensorflow.python.keras.callbacks import TensorBoard\n",
    "from tensorflow.python.keras.losses import mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## import data and tailor it for use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "import pickle\n",
    "with open(\"data\", \"rb\") as f:\n",
    "    df_X = pickle.load(f)\n",
    "    df_Y = pickle.load(f)\n",
    "\n",
    "shift_iter = 24\n",
    "X = df_X.values[shift_iter + 1:-shift_iter - 1]\n",
    "Y = df_Y.values[shift_iter + 1:-shift_iter - 1]\n",
    "# print(X.shape)\n",
    "# print(Y.shape)\n",
    "# print(list(df_X),list(df_Y))\n",
    "\n",
    "ratio = 0.75  # for spliting train - test set\n",
    "num_train = round(X.shape[0] * ratio)\n",
    "x_train = X[:num_train]\n",
    "y_train = Y[:num_train]\n",
    "x_test = X[num_train:]\n",
    "y_test = Y[num_train:]\n",
    "\n",
    "x_scaler = MinMaxScaler()\n",
    "x_train_scaled = x_scaler.fit_transform(x_train)\n",
    "x_test_scaled = x_scaler.transform(x_test)\n",
    "\n",
    "y_scaler = MinMaxScaler()\n",
    "y_train_scaled = y_scaler.fit_transform(y_train)\n",
    "y_test_scaled = y_scaler.transform(y_test)\n",
    "\n",
    "validation_data = (np.expand_dims(x_test_scaled, axis=0),\n",
    "                   np.expand_dims(y_test_scaled, axis=0))\n",
    "\n",
    "num_x_signals = X.shape[1]\n",
    "num_y_signals = Y.shape[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_generator(batch_size, sequence_length):\n",
    "    \"\"\"\n",
    "    Generator function for creating random batches of training-data.\n",
    "    You need to prepare the data before running this x_train_scaled , y_train_scaled\n",
    "    \"\"\"\n",
    "\n",
    "    # Infinite loop.\n",
    "    while True:\n",
    "        # Allocate a new array for the batch of input-signals.\n",
    "        x_shape = (batch_size, sequence_length, num_x_signals)\n",
    "        x_batch = np.zeros(shape=x_shape, dtype=np.float16)\n",
    "\n",
    "        # Allocate a new array for the batch of output-signals.\n",
    "        y_shape = (batch_size, sequence_length, num_y_signals)\n",
    "        y_batch = np.zeros(shape=y_shape, dtype=np.float16)\n",
    "\n",
    "        # Fill the batch with random sequences of data.\n",
    "        for i in range(batch_size):\n",
    "            # Get a random start-index.\n",
    "            # This points somewhere into the training-data.\n",
    "            idx = np.random.randint(num_train - sequence_length)\n",
    "\n",
    "            # Copy the sequences of data starting at this index.\n",
    "            x_batch[i] = x_train_scaled[idx:idx + sequence_length]\n",
    "            y_batch[i] = y_train_scaled[idx:idx + sequence_length]\n",
    "\n",
    "        yield (x_batch, y_batch)\n",
    "\n",
    "        \n",
    "class TensorBoard_ValidateScalar(TensorBoard):\n",
    "    def __init__(self, log_dir='./logs_for_tensorboard', **kwargs):\n",
    "        \n",
    "        # Make the original `TensorBoard` log to a subdirectory 'training'\n",
    "        training_log_dir = os.path.join(log_dir, 'training')\n",
    "        super().__init__(training_log_dir, **kwargs)\n",
    "\n",
    "        # Log the validation metrics to a separate subdirectory\n",
    "        self.val_log_dir = os.path.join(log_dir, 'validation')\n",
    "\n",
    "    def set_model(self, model):\n",
    "        # Setup writer for validation metrics\n",
    "        self.val_writer = tf.summary.FileWriter(self.val_log_dir)\n",
    "        super().set_model(model)\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        # Pop the validation logs and handle them separately with\n",
    "        # `self.val_writer`. Also rename the keys so that they can\n",
    "        # be plotted on the same figure with the training metrics\n",
    "        logs = logs or {}\n",
    "        val_logs = {k.replace('val_', ''): v for k, v in logs.items() if k.startswith('val_')}\n",
    "        for name, value in val_logs.items():\n",
    "            summary = tf.Summary(value=[tf.Summary.Value(tag=name, simple_value=value)])\n",
    "            self.val_writer.add_summary(summary, epoch)\n",
    "        self.val_writer.flush()\n",
    "\n",
    "        # Pass the remaining logs to `TensorBoard.on_epoch_end`\n",
    "        train_logs = {k: v for k, v in logs.items() if not k.startswith('val_')}\n",
    "        super().on_epoch_end(epoch, train_logs) # default send into scalars \n",
    "        \n",
    "    def on_train_end(self, logs=None):\n",
    "        super().on_train_end(logs)\n",
    "        self.val_writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural network on Keras (Tensorflow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(num_x_signals ,\n",
    "              activation='hard_sigmoid',\n",
    "              name = \"hidden\",\n",
    "              input_shape=(None, num_x_signals,)))\n",
    "\n",
    "model.add(Dense(num_y_signals, \n",
    "                activation='linear',\n",
    "                name = \"output\"))\n",
    "run_metadata= tf.RunMetadata()\n",
    "model.compile(\n",
    "          loss = mean_squared_error,\n",
    "          optimizer = Adam(lr=1e-2),\n",
    "          metrics = ['accuracy','mae','mean_squared_error','mape'], # this part will show up in tensorboard\n",
    "          options = tf.RunOptions(trace_level = tf.RunOptions.FULL_TRACE), \n",
    "          run_metadata= tf.RunMetadata(), # these two show up in Chrome Trace\n",
    "             )\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir=\"./logs_tensorboard_demo/{}\".format(arrow.now().format('YYYY-MM-DD-HH-MM-SS'))\n",
    "print(log_dir)\n",
    "\n",
    "\n",
    "model.fit_generator(generator = batch_generator(batch_size=200, sequence_length=168) ,\n",
    "                    epochs=10,\n",
    "                    verbose = 1,\n",
    "                    steps_per_epoch=80,\n",
    "                    validation_data=validation_data,\n",
    "                    callbacks=[\n",
    "                           TensorBoard_ValidateScalar(\n",
    "                           log_dir,\n",
    "                           histogram_freq = 1,\n",
    "                           write_graph = True, # visualize the graph in TensorBoard\n",
    "                           #write_images = True, #  write model weight&bias to visualize as image in TensorBoard\n",
    "                           write_grads = True, # visualize training gradient histograms in TensorBoard\n",
    "                            )])\n",
    "print('Run `tensorboard --logdir=%s` to see the results.' % log_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tracing\n",
    "####  Use chrome and open link chrome://tracing , load trace file named as timeline1.ctf.json to check the data running time and flow direction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trace = timeline.Timeline(step_stats = run_metadata.step_stats)\n",
    "with open(log_dir+'/timeline1.ctf.json','w') as f:\n",
    "    f.write(trace.generate_chrome_trace_format())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural network on Tensorflow\n",
    "tf.summary.\n",
    "\n",
    "* scalar(...): Outputs a Summary protocol buffer containing a single scalar value.\n",
    "        \n",
    "* histogram(...): Outputs a Summary protocol buffer with a histogram.\n",
    "        \n",
    "* audio(...): Outputs a Summary protocol buffer with audio.\n",
    "* image(...): Outputs a Summary protocol buffer with images.\n",
    "* text(...): Summarizes textual data.\n",
    "        \n",
    "* merge(...): Merges summaries\n",
    "* merge_all(...): Merges all summaries collected in the default graph.\n",
    "        \n",
    "* tensor_summary(...): Outputs a Summary protocol buffer with a serialized tensor.proto.\n",
    "* get_summary_description(...): Given a TensorSummary node_def, retrieve its SummaryDescription.\n",
    "\n",
    "``` \n",
    "    -------- You can start code like -------- \n",
    "    with tf.summary.FileWriter(log_dir) as writer: \n",
    "        do whatever\n",
    "       \n",
    "   --------OR --------\n",
    "   writer = tf.summary.FileWriter(log_dir)\n",
    "   do whatever \n",
    "   writer.close()\n",
    "      \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import arrow\n",
    "\n",
    "log_dir=\"./logs_tensorboard_demo/{}\".format(arrow.now().format('YYYY-MM-DD-HH-MM-SS'))\n",
    "\n",
    "# making up some random dataset\n",
    "x_data = np.linspace(-1,1,300)[:,np.newaxis] # flatten your arrary into list\n",
    "noise = np.random.normal(0,0.05,x_data.shape)\n",
    "y_data = np.square(x_data)-0.5+noise\n",
    "\n",
    "# Clears the default graph stack and resets the global default graph \n",
    "tf.reset_default_graph()\n",
    "\n",
    "# tf.summary.image('input',tf.reshape(x_data,[1,300,1,1]))\n",
    "with tf.name_scope('input_layer'):\n",
    "    x = tf.placeholder(tf.float32,[None,1],name='x_input')\n",
    "    y = tf.placeholder(tf.float32,[None,1],name='y_input')\n",
    "\n",
    "    \n",
    "with tf.name_scope('hidden_layer'):\n",
    "    with tf.name_scope('W'):\n",
    "        W1 = tf.Variable(tf.random_normal([1,10]))\n",
    "        tf.summary.histogram('hidden-weight',W1)\n",
    "\n",
    "    with tf.name_scope('B'):\n",
    "        b1 = tf.constant(0.5,shape=[1,10])\n",
    "        tf.summary.histogram('hidden-bias',b1)\n",
    "\n",
    "    with tf.name_scope('Activation'):\n",
    "        a = tf.matmul(x,W1)+b1\n",
    "        tf.summary.histogram('hidden-pre-active',a)\n",
    "\n",
    "    hidden = tf.nn.softmax(a)\n",
    "    tf.summary.histogram('hidden-activation',hidden)\n",
    "\n",
    "with tf.name_scope('output_layer'):\n",
    "    with tf.name_scope('W'):\n",
    "        W2 = tf.Variable(tf.random_normal([10,1]))\n",
    "        tf.summary.histogram('output-weight', W2)\n",
    "\n",
    "    with tf.name_scope('B'):\n",
    "        b2 = tf.constant(0.1, shape=[1, 1])\n",
    "        tf.summary.histogram('output-bias', b2)\n",
    "\n",
    "    with tf.name_scope('Activation'):\n",
    "        a = tf.matmul(hidden, W2) + b2\n",
    "        tf.summary.histogram('output-pre-active', a)\n",
    "\n",
    "        output = tf.nn.relu(a)\n",
    "        tf.summary.histogram('output-activation', output)\n",
    "\n",
    "\n",
    "with tf.name_scope('loss'):\n",
    "    loss = tf.reduce_mean(tf.reduce_sum(tf.square(y-output),reduction_indices=[1]))\n",
    "    tf.summary.scalar('loss',loss)\n",
    "\n",
    "\n",
    "with tf.name_scope('train'):\n",
    "    train = tf.train.GradientDescentOptimizer(0.1).minimize(loss)\n",
    "\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "merge = tf.summary.merge_all()\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    sess.run(init)\n",
    "    writer = tf.summary.FileWriter(log_dir,sess.graph)\n",
    "\n",
    "    for i in range(500) :\n",
    "        sess.run(train,feed_dict={x:x_data,y:y_data})\n",
    "        result = sess.run(merge,feed_dict={x:x_data,y:y_data})\n",
    "        writer.add_summary(result,i)\n",
    "        \n",
    "    writer.close()\n",
    "print(log_dir)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding projector in Tensorboard [Demo](https://projector.tensorflow.org/)\n",
    "Demo from official tensorboard \n",
    "https://github.com/decentralion/tf-dev-summit-tensorboard-tutorial/blob/master/mnist.py \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import os.path\n",
    "import tensorflow as tf\n",
    "import arrow\n",
    "LOGDIR=\"./tmp/mnist_tutorial/{}/\".format(arrow.now().format('YYYY-MM-DD-HH-MM-SS'))\n",
    "\n",
    "# You need to download these two files from the github link and put in the current folder same as notebook \n",
    "LABELS = os.path.join(os.getcwd(), \"labels_1024.tsv\")\n",
    "SPRITES = os.path.join(os.getcwd(), \"sprite_1024.png\")\n",
    "\n",
    "### MNIST EMBEDDINGS ###\n",
    "mnist = tf.contrib.learn.datasets.mnist.read_data_sets(train_dir=LOGDIR + \"data\", one_hot=True)\n",
    "\n",
    "### Get a sprite and labels file for the embedding projector ###\n",
    "\n",
    "if not (os.path.isfile(LABELS) and os.path.isfile(SPRITES)):\n",
    "    print(\"Necessary data files were not found. Run this command from inside the \"\n",
    "          \"repo provided at \"\n",
    "          \"https://github.com/dandelionmane/tf-dev-summit-tensorboard-tutorial.\")\n",
    "    os.exit(1)\n",
    "\n",
    "\n",
    "# shutil.copyfile(LABELS, os.path.join(LOGDIR, LABELS))\n",
    "# shutil.copyfile(SPRITES, os.path.join(LOGDIR, SPRITES))\n",
    "\n",
    "\n",
    "def conv_layer(input, size_in, size_out, name=\"conv\"):\n",
    "    with tf.name_scope(name):\n",
    "        w = tf.Variable(tf.truncated_normal([5, 5, size_in, size_out], stddev=0.1), name=\"W\")\n",
    "        b = tf.Variable(tf.constant(0.1, shape=[size_out]), name=\"B\")\n",
    "        conv = tf.nn.conv2d(input, w, strides=[1, 1, 1, 1], padding=\"SAME\")\n",
    "        act = tf.nn.relu(conv + b)\n",
    "        tf.summary.histogram(\"weights\", w)\n",
    "        tf.summary.histogram(\"biases\", b)\n",
    "        tf.summary.histogram(\"activations\", act)\n",
    "        return tf.nn.max_pool(act, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding=\"SAME\")\n",
    "\n",
    "\n",
    "def fc_layer(input, size_in, size_out, name=\"fc\"):\n",
    "    with tf.name_scope(name):\n",
    "        w = tf.Variable(tf.truncated_normal([size_in, size_out], stddev=0.1), name=\"W\")\n",
    "        b = tf.Variable(tf.constant(0.1, shape=[size_out]), name=\"B\")\n",
    "        act = tf.matmul(input, w) + b\n",
    "        tf.summary.histogram(\"weights\", w)\n",
    "        tf.summary.histogram(\"biases\", b)\n",
    "        tf.summary.histogram(\"activations\", act)\n",
    "        return act\n",
    "\n",
    "\n",
    "def mnist_model(learning_rate, use_two_fc, use_two_conv, hparam):\n",
    "    # reset and clean the default graph in stack\n",
    "    tf.reset_default_graph()\n",
    "    with tf.Session() as sess:\n",
    "        \n",
    "        # Setup placeholders, and reshape the data\n",
    "        x = tf.placeholder(tf.float32, shape=[None, 784], name=\"x\")\n",
    "        x_image = tf.reshape(x, [-1, 28, 28, 1])\n",
    "        tf.summary.image('input', x_image,max_outputs=10)\n",
    "        y = tf.placeholder(tf.float32, shape=[None, 10], name=\"labels\")\n",
    "\n",
    "        if use_two_conv:\n",
    "            conv1 = conv_layer(x_image, 1, 32, \"conv1\")\n",
    "            conv_out = conv_layer(conv1, 32, 64, \"conv2\")\n",
    "        else:\n",
    "            conv_out = conv_layer(x_image, 1, 16, \"conv\")\n",
    "\n",
    "        flattened = tf.reshape(conv_out, [-1, 7 * 7 * 64])\n",
    "\n",
    "        if use_two_fc:\n",
    "            fc1 = fc_layer(flattened, 7 * 7 * 64, 1024, \"fc1\")\n",
    "            relu = tf.nn.relu(fc1)\n",
    "            embedding_input = relu\n",
    "            tf.summary.histogram(\"fc1/relu\", relu)\n",
    "            embedding_size = 1024\n",
    "            logits = fc_layer(relu, 1024, 10, \"fc2\")\n",
    "        else:\n",
    "            embedding_input = flattened\n",
    "            embedding_size = 7 * 7 * 64\n",
    "            logits = fc_layer(flattened, 7 * 7 * 64, 10, \"fc\")\n",
    "\n",
    "        with tf.name_scope(\"loss\"):\n",
    "            xent = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
    "                    logits=logits, labels=y), name=\"loss\")\n",
    "            tf.summary.scalar(\"loss\", xent)\n",
    "\n",
    "        with tf.name_scope(\"train\"):\n",
    "            train_step = tf.train.AdamOptimizer(learning_rate).minimize(xent)\n",
    "\n",
    "        with tf.name_scope(\"accuracy\"):\n",
    "            correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(y, 1))\n",
    "            accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "            tf.summary.scalar(\"accuracy\", accuracy)\n",
    "\n",
    "        summ = tf.summary.merge_all()\n",
    "\n",
    "        embedding = tf.Variable(tf.zeros([1024, embedding_size]), name=\"test_embedding\")\n",
    "        assignment = embedding.assign(embedding_input)\n",
    "        \n",
    "        # The embedding projector will read the embeddings from your model checkpoint file\n",
    "        saver = tf.train.Saver()\n",
    "\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        writer = tf.summary.FileWriter(LOGDIR + hparam)\n",
    "        writer.add_graph(sess.graph)\n",
    "\n",
    "        config = tf.contrib.tensorboard.plugins.projector.ProjectorConfig()\n",
    "        embedding_config = config.embeddings.add()\n",
    "        embedding_config.tensor_name = embedding.name\n",
    "        embedding_config.sprite.image_path = SPRITES\n",
    "        embedding_config.metadata_path = LABELS\n",
    "        # Specify the width and height of a single thumbnail.\n",
    "        embedding_config.sprite.single_image_dim.extend([28, 28])\n",
    "        tf.contrib.tensorboard.plugins.projector.visualize_embeddings(writer, config)\n",
    "\n",
    "        for i in range(2001):\n",
    "            batch = mnist.train.next_batch(100)\n",
    "            if i % 5 == 0:\n",
    "                [train_accuracy, s] = sess.run([accuracy, summ], feed_dict={x: batch[0], y: batch[1]})\n",
    "                writer.add_summary(s, i)\n",
    "            if i % 500 == 0:\n",
    "                sess.run(assignment, feed_dict={x: mnist.test.images[:1024], y: mnist.test.labels[:1024]})\n",
    "                saver.save(sess, os.path.join(LOGDIR, \"model.ckpt\"), i)\n",
    "            sess.run(train_step, feed_dict={x: batch[0], y: batch[1]})\n",
    "    \n",
    "\n",
    "def make_hparam_string(learning_rate, use_two_fc, use_two_conv):\n",
    "    conv_param = \"conv=2\" if use_two_conv else \"conv=1\"\n",
    "    fc_param = \"fc=2\" if use_two_fc else \"fc=1\"\n",
    "    return \"lr_%.0E,%s,%s\" % (learning_rate, conv_param, fc_param)\n",
    "\n",
    "\n",
    "def main():\n",
    "    # You can try adding some more learning rates\n",
    "    for learning_rate in [1E-3]: #, 1E-4\n",
    "        # Include \"False\" as a value to try different model architectures\n",
    "        for use_two_fc in [False]:\n",
    "            for use_two_conv in [False]: #, True\n",
    "                # Construct a hyperparameter string for each one (example: \"lr_1E-3,fc=2,conv=2\")\n",
    "                hparam = make_hparam_string(learning_rate, use_two_fc, use_two_conv)\n",
    "                print('Starting run for %s' % hparam)\n",
    "\n",
    "                # Actually run with the new settings\n",
    "                mnist_model(learning_rate, use_two_fc, use_two_conv, hparam)\n",
    "    print('Done training!')\n",
    "    print('Run `tensorboard --logdir=%s` to see the results.' % LOGDIR)\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ref \n",
    "- [t-SNE](https://distill.pub/2016/misread-tsne/)\n",
    "\n",
    "    A popular method for exploring high-dimensional data is something called t-SNE, introduced by van der Maaten and Hinton in 2008. \n",
    "    \n",
    "    The technique has become widespread in the field of machine learning, since it has an almost magical ability to create compelling two-dimensonal “maps” from data with hundreds or even thousands of dimensions. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
