{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tersor....Board....ing "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Watching the [introduction video](https://www.youtube.com/watch?v=eBbEDRsCmv4&t=1105s) [Demo code](https://github.com/decentralion/tf-dev-summit-tensorboard-tutorial)\n",
    "\n",
    "If (Tensorboard == useful): \n",
    "    Install tensorflow https://www.tensorflow.org/install   \n",
    "else \n",
    "    Close this notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### start the tensorboard : \n",
    "\n",
    "> /Prometheus/src$ tensorboard --logdir=./logs_ann/\n",
    "> \n",
    "> TensorBoard 1.8.0 at __http://*****:6006__ (Press CTRL+C to quit)\n",
    "> \n",
    "\n",
    "Tips: if you open the 6006 url link but find nothing, calm down, double check the path. Wrong folder will give you empty board \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Wrong code could too, so the code begins here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL']='2'\n",
    "\n",
    "from os import listdir\n",
    "from tensorflow.python.client import timeline\n",
    "\n",
    "import calendar\n",
    "import time\n",
    "import arrow\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.python.keras.models import Sequential\n",
    "from tensorflow.python.keras.layers import Dense, GRU,Input\n",
    "from tensorflow.python.keras.optimizers import *\n",
    "from tensorflow.python.keras.callbacks import TensorBoard\n",
    "from tensorflow.python.keras.losses import mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## import data and tailor it for use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"data\", \"rb\") as f:\n",
    "    df_X = pickle.load(f)\n",
    "    df_Y = pickle.load(f)\n",
    "\n",
    "shift_iter = 24\n",
    "X = df_X.values[shift_iter + 1:-shift_iter - 1]\n",
    "Y = df_Y.values[shift_iter + 1:-shift_iter - 1]\n",
    "# print(X.shape)\n",
    "# print(Y.shape)\n",
    "# print(list(df_X),list(df_Y))\n",
    "\n",
    "ratio = 0.75  # for spliting train - test set\n",
    "num_train = round(X.shape[0] * ratio)\n",
    "x_train = X[:num_train]\n",
    "y_train = Y[:num_train]\n",
    "x_test = X[num_train:]\n",
    "y_test = Y[num_train:]\n",
    "\n",
    "x_scaler = MinMaxScaler()\n",
    "x_train_scaled = x_scaler.fit_transform(x_train)\n",
    "x_test_scaled = x_scaler.transform(x_test)\n",
    "\n",
    "y_scaler = MinMaxScaler()\n",
    "y_train_scaled = y_scaler.fit_transform(y_train)\n",
    "y_test_scaled = y_scaler.transform(y_test)\n",
    "\n",
    "validation_data = (np.expand_dims(x_test_scaled, axis=0),\n",
    "                   np.expand_dims(y_test_scaled, axis=0))\n",
    "\n",
    "num_x_signals = X.shape[1]\n",
    "num_y_signals = Y.shape[1]\n",
    "\n",
    "def batch_generator(batch_size, sequence_length):\n",
    "    \"\"\"\n",
    "    Generator function for creating random batches of training-data.\n",
    "    You need to prepare the data before running this x_train_scaled , y_train_scaled\n",
    "    \"\"\"\n",
    "\n",
    "    # Infinite loop.\n",
    "    while True:\n",
    "        # Allocate a new array for the batch of input-signals.\n",
    "        x_shape = (batch_size, sequence_length, num_x_signals)\n",
    "        x_batch = np.zeros(shape=x_shape, dtype=np.float16)\n",
    "\n",
    "        # Allocate a new array for the batch of output-signals.\n",
    "        y_shape = (batch_size, sequence_length, num_y_signals)\n",
    "        y_batch = np.zeros(shape=y_shape, dtype=np.float16)\n",
    "\n",
    "        # Fill the batch with random sequences of data.\n",
    "        for i in range(batch_size):\n",
    "            # Get a random start-index.\n",
    "            # This points somewhere into the training-data.\n",
    "            idx = np.random.randint(num_train - sequence_length)\n",
    "\n",
    "            # Copy the sequences of data starting at this index.\n",
    "            x_batch[i] = x_train_scaled[idx:idx + sequence_length]\n",
    "            y_batch[i] = y_train_scaled[idx:idx + sequence_length]\n",
    "\n",
    "        yield (x_batch, y_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create a model from tensorflow using API : Keras "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TensorBoard_ValidateScalar(TensorBoard):\n",
    "    def __init__(self, log_dir='./logs_for_tensorboard', **kwargs):\n",
    "        \n",
    "        # Make the original `TensorBoard` log to a subdirectory 'training'\n",
    "        training_log_dir = os.path.join(log_dir, 'training')\n",
    "        super().__init__(training_log_dir, **kwargs)\n",
    "\n",
    "        # Log the validation metrics to a separate subdirectory\n",
    "        self.val_log_dir = os.path.join(log_dir, 'validation')\n",
    "\n",
    "    def set_model(self, model):\n",
    "        # Setup writer for validation metrics\n",
    "        self.val_writer = tf.summary.FileWriter(self.val_log_dir)\n",
    "        super().set_model(model)\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        # Pop the validation logs and handle them separately with\n",
    "        # `self.val_writer`. Also rename the keys so that they can\n",
    "        # be plotted on the same figure with the training metrics\n",
    "        logs = logs or {}\n",
    "        val_logs = {k.replace('val_', ''): v for k, v in logs.items() if k.startswith('val_')}\n",
    "        for name, value in val_logs.items():\n",
    "            summary = tf.Summary(value=[tf.Summary.Value(tag=name, simple_value=value)])\n",
    "            self.val_writer.add_summary(summary, epoch)\n",
    "        self.val_writer.flush()\n",
    "\n",
    "        # Pass the remaining logs to `TensorBoard.on_epoch_end`\n",
    "        train_logs = {k: v for k, v in logs.items() if not k.startswith('val_')}\n",
    "        super().on_epoch_end(epoch, train_logs) # default send into scalars \n",
    "        \n",
    "    def on_train_end(self, logs=None):\n",
    "        super().on_train_end(logs)\n",
    "        self.val_writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TensorBoard_ValidateHistogram(TensorBoard):\n",
    "    def __init__(self, log_dir='./logs_for_tensorboard', **kwargs):\n",
    "        # Make the original `TensorBoard` log to a subdirectory 'training'\n",
    "        self.training_log_dir = os.path.join(log_dir, 'training')\n",
    "        super().__init__(self.training_log_dir, **kwargs)\n",
    "\n",
    "        # Log the validation metrics to a separate subdirectory\n",
    "        self.val_log_dir = os.path.join(log_dir, 'validation')\n",
    "\n",
    "    def set_model(self, model):\n",
    "        # Setup writer for validation metrics\n",
    "        self.val_writer = tf.summary.FileWriter(self.val_log_dir)\n",
    "        self.train_writer = tf.summary.FileWriter(self.training_log_dir)\n",
    "        super().set_model(model)\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        print(\"\\n\",logs,\"\\n\")\n",
    "        # Pop the validation logs and handle them separately with\n",
    "        # `self.val_writer`. Also rename the keys so that they can\n",
    "        # be plotted on the same figure with the training metrics\n",
    "        logs = logs or {}\n",
    "        val_logs = {k.replace('val_', ''): v for k, v in logs.items() if k.startswith('val_')}\n",
    "        for name, value in val_logs.items():\n",
    "            \n",
    "            # histogram in Tensorboard\n",
    "            values = np.array(value)\n",
    "            # Create histogram using numpy\n",
    "            counts, bin_edges = np.histogram(values, bins=1000)\n",
    "            # Fill fields of histogram proto\n",
    "            hist = tf.HistogramProto()\n",
    "            hist.min = float(np.min(values))\n",
    "            hist.max = float(np.max(values))\n",
    "            hist.num = int(np.prod(values.shape))\n",
    "            hist.sum = float(np.sum(values))\n",
    "            hist.sum_squares = float(np.sum(values ** 2))\n",
    "            bin_edges = bin_edges[1:]\n",
    "\n",
    "            # Add bin edges and counts\n",
    "            for edge in bin_edges:\n",
    "                hist.bucket_limit.append(edge)\n",
    "            for c in counts:\n",
    "                hist.bucket.append(c)\n",
    "            summary = tf.Summary(value=[tf.Summary.Value(tag=name, histo=hist)])\n",
    "            \n",
    "            self.val_writer.add_summary(summary, epoch)\n",
    "\n",
    "        self.val_writer.flush()\n",
    "\n",
    "        # Pass the train logs to `TensorBoard.on_epoch_end`\n",
    "        train_logs = {k: v for k, v in logs.items() if not k.startswith('val_')}\n",
    "        for name, value in train_logs.items():\n",
    "            \n",
    "            # histogram in Tensorboard\n",
    "            values = np.array(value)\n",
    "            # Create histogram using numpy\n",
    "            counts, bin_edges = np.histogram(values, bins=1000)\n",
    "            # Fill fields of histogram proto\n",
    "            hist = tf.HistogramProto()\n",
    "            hist.min = float(np.min(values))\n",
    "            hist.max = float(np.max(values))\n",
    "            hist.num = int(np.prod(values.shape))\n",
    "            hist.sum = float(np.sum(values))\n",
    "            hist.sum_squares = float(np.sum(values ** 2))\n",
    "            bin_edges = bin_edges[1:]\n",
    "\n",
    "            # Add bin edges and counts\n",
    "            for edge in bin_edges:\n",
    "                hist.bucket_limit.append(edge)\n",
    "            for c in counts:\n",
    "                hist.bucket.append(c)\n",
    "            summary = tf.Summary(value=[tf.Summary.Value(tag=name, histo=hist)])\n",
    "            self.train_writer.add_summary(summary, epoch)\n",
    "        self.train_writer.flush()\n",
    "        \n",
    "        # write the gradient and all the metrics values into tensorboard \n",
    "        super().on_epoch_end(epoch, logs) \n",
    "\n",
    "    def on_train_end(self, logs=None):\n",
    "        super().on_train_end(logs)\n",
    "        self.val_writer.close()\n",
    "        self.train_writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(num_x_signals ,\n",
    "              activation='hard_sigmoid',\n",
    "              name = \"hidden\",\n",
    "              input_shape=(None, num_x_signals,)))\n",
    "\n",
    "model.add(Dense(num_y_signals, \n",
    "                activation='linear',\n",
    "                name = \"output\"))\n",
    "\n",
    "model.compile(loss = mean_squared_error,\n",
    "          optimizer = Adam(lr=1e-2),\n",
    "          metrics = ['accuracy','mae','mean_squared_error','mape'], # this part will show up in tensorboard\n",
    "          options = tf.RunOptions(trace_level = tf.RunOptions.FULL_TRACE), \n",
    "          run_metadata= tf.RunMetadata(), # these two show up in Chrome Trace\n",
    "             )\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "log_dir=\"./logs_tensorboard_demo/{}\".format(arrow.now().format('YYYY-MM-DD-HH-MM-SS'))\n",
    "print(log_dir)\n",
    "\n",
    "model.fit_generator(generator = batch_generator(batch_size=200, sequence_length=168) ,\n",
    "                    epochs=10,\n",
    "                    verbose = 0,\n",
    "                    steps_per_epoch=80,\n",
    "                    validation_data=validation_data,\n",
    "                    callbacks=[\n",
    "                           TensorBoard_ValidateHistogram(\n",
    "                           log_dir,\n",
    "                           histogram_freq = 1,\n",
    "                           write_graph = True, # visualize the graph in TensorBoard\n",
    "                           write_images = True, #  write model weights to visualize as image in TensorBoard\n",
    "                           write_grads = True, # visualize training gradient histograms in TensorBoard\n",
    "                                  )\n",
    "                            ])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "trace = timeline.Timeline(step_stats = run_metadata.step_stats)\n",
    "with open('timeline1.ctf.json','w') as f:\n",
    "    f.write(trace.generate_chrome_trace_format())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os._exit(00)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import arrow\n",
    "\n",
    "log_dir=\"./logs_tensorboard_demo/{}\".format(arrow.now().format('YYYY-MM-DD-HH-MM-SS'))\n",
    "\n",
    "\n",
    "\n",
    "# making up some random data\n",
    "x_data = np.linspace(-1,1,300)[:,np.newaxis] # flatten your arrary into list\n",
    "noise = np.random.normal(0,0.05,x_data.shape)\n",
    "y_data = np.square(x_data)-0.5+noise\n",
    "\n",
    "# tf.summary.image('input',tf.reshape(x_data,[1,300,1,1]))\n",
    "with tf.name_scope('input_layer'):\n",
    "    x = tf.placeholder(tf.float32,[None,1],name='x_input')\n",
    "    y = tf.placeholder(tf.float32,[None,1],name='y_input')\n",
    "\n",
    "    \n",
    "with tf.name_scope('hidden_layer'):\n",
    "    with tf.name_scope('W'):\n",
    "        W1 = tf.Variable(tf.random_normal([1,10]))\n",
    "        tf.summary.histogram('hidden-weight',W1)\n",
    "\n",
    "    with tf.name_scope('B'):\n",
    "        b1 = tf.constant(0.5,shape=[1,10])\n",
    "        tf.summary.histogram('hidden-bias',b1)\n",
    "\n",
    "    with tf.name_scope('Activation'):\n",
    "        a = tf.matmul(x,W1)+b1\n",
    "        tf.summary.histogram('hidden-pre-active',a)\n",
    "\n",
    "    hidden = tf.nn.softmax(a)\n",
    "    tf.summary.histogram('hidden-activation',hidden)\n",
    "\n",
    "with tf.name_scope('output_layer'):\n",
    "    with tf.name_scope('W'):\n",
    "        W2 = tf.Variable(tf.random_normal([10,1]))\n",
    "        tf.summary.histogram('output-weight', W2)\n",
    "\n",
    "    with tf.name_scope('B'):\n",
    "        b2 = tf.constant(0.1, shape=[1, 1])\n",
    "        tf.summary.histogram('output-bias', b2)\n",
    "\n",
    "    with tf.name_scope('Activation'):\n",
    "        a = tf.matmul(hidden, W2) + b2\n",
    "        tf.summary.histogram('output-pre-active', a)\n",
    "\n",
    "    output = (a)\n",
    "    tf.summary.histogram('output-activation', output)\n",
    "\n",
    "\n",
    "with tf.name_scope('loss'):\n",
    "    loss = tf.reduce_mean(tf.reduce_sum(tf.square(y-output),reduction_indices=[1]))\n",
    "    tf.summary.scalar('loss',loss)\n",
    "\n",
    "\n",
    "with tf.name_scope('train'):\n",
    "    train = tf.train.GradientDescentOptimizer(0.1).minimize(loss)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "merge = tf.summary.merge_all()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    writer = tf.summary.FileWriter(log_dir,sess.graph)\n",
    "\n",
    "    for i in range(500) :\n",
    "        sess.run(train,feed_dict={x:x_data,y:y_data})\n",
    "        result = sess.run(merge,feed_dict={x:x_data,y:y_data})\n",
    "        writer.add_summary(result,i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from io import StringIO\n",
    "class Logger(object):\n",
    "    \"\"\"Logging in tensorboard without tensorflow ops.\"\"\"\n",
    "\n",
    "    def __init__(self, log_dir):\n",
    "        \"\"\"Creates a summary writer logging to log_dir.\"\"\"\n",
    "        self.writer = tf.summary.FileWriter(log_dir)\n",
    "\n",
    "    def log_scalar(self, tag, value, step):\n",
    "        \"\"\"Log a scalar variable.\n",
    "        Parameter\n",
    "        ----------\n",
    "        tag : basestring\n",
    "            Name of the scalar\n",
    "        value\n",
    "        step : int\n",
    "            training iteration\n",
    "        \"\"\"\n",
    "        summary = tf.Summary(value=[tf.Summary.Value(tag=tag,\n",
    "                                                     simple_value=value)])\n",
    "        self.writer.add_summary(summary, step)\n",
    "        self.writer.flush()\n",
    "    def log_images(self, tag, images, step):\n",
    "        \"\"\"Logs a list of images.\"\"\"\n",
    "\n",
    "        im_summaries = []\n",
    "        for nr, img in enumerate(images):\n",
    "            # Write the image to a string\n",
    "            s = StringIO()\n",
    "            plt.imsave(s, img, format='png')\n",
    "\n",
    "            # Create an Image object\n",
    "            img_sum = tf.Summary.Image(encoded_image_string=s.getvalue(),\n",
    "                                       height=img.shape[0],\n",
    "                                       width=img.shape[1])\n",
    "            # Create a Summary value\n",
    "            im_summaries.append(tf.Summary.Value(tag='%s/%d' % (tag, nr),\n",
    "                                                 image=img_sum))\n",
    "\n",
    "        # Create and write Summary\n",
    "        summary = tf.Summary(value=im_summaries)\n",
    "        self.writer.add_summary(summary, step)\n",
    "        self.writer.flush()\n",
    "    def log_histogram(self, tag, values, step, bins=1000):\n",
    "        \"\"\"Logs the histogram of a list/vector of values.\"\"\"\n",
    "        # Convert to a numpy array\n",
    "        values = np.array(values)\n",
    "\n",
    "        # Create histogram using numpy\n",
    "        counts, bin_edges = np.histogram(values, bins=bins)\n",
    "\n",
    "        # Fill fields of histogram proto\n",
    "        hist = tf.HistogramProto()\n",
    "        hist.min = float(np.min(values))\n",
    "        hist.max = float(np.max(values))\n",
    "        hist.num = int(np.prod(values.shape))\n",
    "        hist.sum = float(np.sum(values))\n",
    "        hist.sum_squares = float(np.sum(values ** 2))\n",
    "\n",
    "        # Requires equal number as bins, where the first goes from -DBL_MAX to bin_edges[1]\n",
    "        # See https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/framework/summary.proto#L30\n",
    "        # Thus, we drop the start of the first bin\n",
    "        bin_edges = bin_edges[1:]\n",
    "\n",
    "        # Add bin edges and counts\n",
    "        for edge in bin_edges:\n",
    "            hist.bucket_limit.append(edge)\n",
    "        for c in counts:\n",
    "            hist.bucket.append(c)\n",
    "\n",
    "        # Create and write Summary\n",
    "        summary = tf.Summary(value=[tf.Summary.Value(tag=tag, histo=hist)])\n",
    "        self.writer.add_summary(summary, step)\n",
    "        self.writer.flush()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
