{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tersor....Board....ing "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Watching the [introduction video Google Summit 2017](https://www.youtube.com/watch?v=eBbEDRsCmv4&t=1105s) or read the [Tensorboard Tutorial](https://github.com/decentralion/tf-dev-summit-tensorboard-tutorial)\n",
    "\n",
    "If (Tensorboard == useful): \n",
    "    Install tensorflow https://www.tensorflow.org/install   \n",
    "else \n",
    "    Close this notebook\n",
    "   \n",
    "#### start the tensorboard : \n",
    "\n",
    "$ tensorboard --logdir=./logs_ann/\n",
    "> \n",
    "> TensorBoard 1.8.0 at __http://*****:6006__ (Press CTRL+C to quit)\n",
    "> \n",
    "\n",
    "Tips: if you open the 6006 url link but find nothing, calm down, double check the path. Wrong folder will give you empty board \n",
    "\n",
    "####  Wrong code could too, so the code begins here\n",
    "#### Basic functions for Tensorboard\n",
    "tf.summary.\n",
    "\n",
    "* scalar(...): Outputs a Summary protocol buffer containing a single scalar value.\n",
    "        \n",
    "* histogram(...): Outputs a Summary protocol buffer with a histogram.\n",
    "        \n",
    "* audio(...): Outputs a Summary protocol buffer with audio.\n",
    "* image(...): Outputs a Summary protocol buffer with images.\n",
    "* text(...): Summarizes textual data.\n",
    "        \n",
    "* merge(...): Merges summaries\n",
    "* merge_all(...): Merges all summaries collected in the default graph.\n",
    "        \n",
    "* tensor_summary(...): Outputs a Summary protocol buffer with a serialized tensor.proto.\n",
    "* get_summary_description(...): Given a TensorSummary node_def, retrieve its SummaryDescription.\n",
    "\n",
    "``` \n",
    "    -------- You can start code like -------- \n",
    "    with tf.summary.FileWriter(log_dir) as writer: \n",
    "        do whatever\n",
    "       \n",
    "   --------OR --------\n",
    "   writer = tf.summary.FileWriter(log_dir)\n",
    "   do whatever \n",
    "   writer.close()\n",
    "      \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import data(from Prometheus) and tailor it for use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL']='2'\n",
    "\n",
    "from os import listdir\n",
    "from tensorflow.python.client import timeline\n",
    "\n",
    "import calendar\n",
    "import time\n",
    "import arrow\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.python.keras.models import Sequential\n",
    "from tensorflow.python.keras.layers import Dense, GRU,Input\n",
    "from tensorflow.python.keras.optimizers import *\n",
    "from tensorflow.python.keras.callbacks import TensorBoard\n",
    "from tensorflow.python.keras.losses import mean_squared_error\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "import pickle\n",
    "with open(\"data\", \"rb\") as f:\n",
    "    df_X = pickle.load(f)\n",
    "    df_Y = pickle.load(f)\n",
    "\n",
    "shift_iter = 24\n",
    "X = df_X.values[shift_iter + 1:-shift_iter - 1]\n",
    "Y = df_Y.values[shift_iter + 1:-shift_iter - 1]\n",
    "# print(X.shape)\n",
    "# print(Y.shape)\n",
    "# print(list(df_X),list(df_Y))\n",
    "\n",
    "ratio = 0.75  # for spliting train - test set\n",
    "num_train = round(X.shape[0] * ratio)\n",
    "x_train = X[:num_train]\n",
    "y_train = Y[:num_train]\n",
    "x_test = X[num_train:]\n",
    "y_test = Y[num_train:]\n",
    "\n",
    "x_scaler = MinMaxScaler()\n",
    "x_train_scaled = x_scaler.fit_transform(x_train)\n",
    "x_test_scaled = x_scaler.transform(x_test)\n",
    "\n",
    "y_scaler = MinMaxScaler()\n",
    "y_train_scaled = y_scaler.fit_transform(y_train)\n",
    "y_test_scaled = y_scaler.transform(y_test)\n",
    "\n",
    "validation_data = (np.expand_dims(x_test_scaled, axis=0),\n",
    "                   np.expand_dims(y_test_scaled, axis=0))\n",
    "\n",
    "num_x_signals = X.shape[1]\n",
    "num_y_signals = Y.shape[1]\n",
    "\n",
    "def batch_generator(batch_size, sequence_length):\n",
    "    \"\"\"\n",
    "    Generator function for creating random batches of training-data.\n",
    "    You need to prepare the data before running this x_train_scaled , y_train_scaled\n",
    "    \"\"\"\n",
    "\n",
    "    # Infinite loop.\n",
    "    while True:\n",
    "        # Allocate a new array for the batch of input-signals.\n",
    "        x_shape = (batch_size, sequence_length, num_x_signals)\n",
    "        x_batch = np.zeros(shape=x_shape, dtype=np.float16)\n",
    "\n",
    "        # Allocate a new array for the batch of output-signals.\n",
    "        y_shape = (batch_size, sequence_length, num_y_signals)\n",
    "        y_batch = np.zeros(shape=y_shape, dtype=np.float16)\n",
    "\n",
    "        # Fill the batch with random sequences of data.\n",
    "        for i in range(batch_size):\n",
    "            # Get a random start-index.\n",
    "            # This points somewhere into the training-data.\n",
    "            idx = np.random.randint(num_train - sequence_length)\n",
    "\n",
    "            # Copy the sequences of data starting at this index.\n",
    "            x_batch[i] = x_train_scaled[idx:idx + sequence_length]\n",
    "            y_batch[i] = y_train_scaled[idx:idx + sequence_length]\n",
    "\n",
    "        yield (x_batch, y_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural network on Keras API in Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TensorBoard_ValidateScalar(TensorBoard):\n",
    "    \"\"\"\n",
    "    Rewrite the part of the Tensorboard function \n",
    "    \"\"\"\n",
    "    def __init__(self, log_dir='./logs_for_tensorboard', **kwargs):\n",
    "        \n",
    "        # Make the original `TensorBoard` log to a subdirectory 'training'\n",
    "        training_log_dir = os.path.join(log_dir, 'training')\n",
    "        super().__init__(training_log_dir, **kwargs)\n",
    "\n",
    "        # Log the validation metrics to a separate subdirectory\n",
    "        self.val_log_dir = os.path.join(log_dir, 'validation')\n",
    "\n",
    "    def set_model(self, model):\n",
    "        # Setup writer for validation metrics\n",
    "        self.val_writer = tf.summary.FileWriter(self.val_log_dir)\n",
    "        super().set_model(model)\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        # Pop the validation logs and handle them separately with\n",
    "        # `self.val_writer`. Also rename the keys so that they can\n",
    "        # be plotted on the same figure with the training metrics\n",
    "        logs = logs or {}\n",
    "        val_logs = {k.replace('val_', ''): v for k, v in logs.items() if k.startswith('val_')}\n",
    "        for name, value in val_logs.items():\n",
    "            summary = tf.Summary(value=[tf.Summary.Value(tag=name, simple_value=value)])\n",
    "            self.val_writer.add_summary(summary, epoch)\n",
    "        self.val_writer.flush()\n",
    "\n",
    "        # Pass the remaining logs to `TensorBoard.on_epoch_end`\n",
    "        train_logs = {k: v for k, v in logs.items() if not k.startswith('val_')}\n",
    "        super().on_epoch_end(epoch, train_logs) # default send into scalars \n",
    "        \n",
    "    def on_train_end(self, logs=None):\n",
    "        self.val_writer.close()\n",
    "        super().on_train_end(logs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "hidden (Dense)               (None, None, 77)          6006      \n",
      "_________________________________________________________________\n",
      "output (Dense)               (None, None, 24)          1872      \n",
      "=================================================================\n",
      "Total params: 7,878\n",
      "Trainable params: 7,878\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(num_x_signals ,\n",
    "              activation='hard_sigmoid',\n",
    "              name = \"hidden\",\n",
    "              input_shape=(None, num_x_signals,)))\n",
    "\n",
    "model.add(Dense(num_y_signals, \n",
    "                activation='linear',\n",
    "                name = \"output\"))\n",
    "run_metadata= tf.RunMetadata()\n",
    "model.compile(\n",
    "          loss = mean_squared_error,\n",
    "          optimizer = Adam(lr=1e-2),\n",
    "          metrics = ['accuracy','mae','mean_squared_error','mape'], # this part will show up in tensorboard\n",
    "          options = tf.RunOptions(trace_level = tf.RunOptions.FULL_TRACE), \n",
    "          run_metadata= run_metadata, # these two show up in Chrome Trace\n",
    "             )\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run `tensorboard --logdir=./logs_tensorboard_demo/2018-08-10-12-08-38` to see the results.\n"
     ]
    }
   ],
   "source": [
    "log_dir=\"./logs_tensorboard_demo/{}\".format(arrow.now().format('YYYY-MM-DD-HH-MM-SS'))\n",
    "\n",
    "model.fit_generator(generator = batch_generator(batch_size=200, sequence_length=168) ,\n",
    "                    epochs=10,\n",
    "                    verbose = 0,\n",
    "                    steps_per_epoch=80,\n",
    "                    validation_data=validation_data,\n",
    "                    callbacks=[\n",
    "                           TensorBoard_ValidateScalar(\n",
    "                           log_dir,\n",
    "                           histogram_freq = 1,\n",
    "                           write_graph = True, # visualize the graph in TensorBoard\n",
    "                           #write_images = True, #  write model weight&bias to visualize as image in TensorBoard\n",
    "                           write_grads = True, # visualize training gradient histograms in TensorBoard\n",
    "                            )])\n",
    "print('Run `tensorboard --logdir=%s` to see the results.' % log_dir)\n",
    "\n",
    "trace = timeline.Timeline(step_stats = run_metadata.step_stats)\n",
    "with open(log_dir+'/timeline.ctf.json','w') as f:\n",
    "    f.write(trace.generate_chrome_trace_format())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Tracing : Open link on [Chrome](chrome://tracing)chrome://tracing, load trace file named as timeline.ctf.json in the demo 1 folder, to check the data running time and flow direction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural network on Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import arrow\n",
    "\n",
    "log_dir=\"./logs_tensorboard_demo/{}/\".format(arrow.now().format('YYYY-MM-DD-HH-MM-SS'))\n",
    "\n",
    "# making up some random dataset\n",
    "x_data = np.linspace(-1,1,300)[:,np.newaxis] # flatten your arrary into list\n",
    "noise = np.random.normal(0,0.05,x_data.shape)\n",
    "y_data = np.square(x_data)-0.5+noise\n",
    "\n",
    "# Clears the default graph stack and resets the global default graph \n",
    "tf.reset_default_graph()\n",
    "\n",
    "# tf.summary.image('input',tf.reshape(x_data,[1,300,1,1]))\n",
    "with tf.name_scope('input_layer'):\n",
    "    x = tf.placeholder(tf.float32,[None,1],name='x_input')\n",
    "    y = tf.placeholder(tf.float32,[None,1],name='y_input')\n",
    "\n",
    "    \n",
    "with tf.name_scope('hidden_layer'):\n",
    "    with tf.name_scope('W'):\n",
    "        W1 = tf.Variable(tf.random_normal([1,10]))\n",
    "        tf.summary.histogram('hidden-weight',W1)\n",
    "\n",
    "    with tf.name_scope('B'):\n",
    "        b1 = tf.constant(0.5,shape=[1,10])\n",
    "        tf.summary.histogram('hidden-bias',b1)\n",
    "\n",
    "    with tf.name_scope('Activation'):\n",
    "        a = tf.matmul(x,W1)+b1\n",
    "        tf.summary.histogram('hidden-pre-active',a)\n",
    "\n",
    "    hidden = tf.nn.softmax(a)\n",
    "    tf.summary.histogram('hidden-activation',hidden)\n",
    "\n",
    "with tf.name_scope('output_layer'):\n",
    "    with tf.name_scope('W'):\n",
    "        W2 = tf.Variable(tf.random_normal([10,1]))\n",
    "        tf.summary.histogram('output-weight', W2)\n",
    "\n",
    "    with tf.name_scope('B'):\n",
    "        b2 = tf.constant(0.1, shape=[1, 1])\n",
    "        tf.summary.histogram('output-bias', b2)\n",
    "\n",
    "    with tf.name_scope('Activation'):\n",
    "        a = tf.matmul(hidden, W2) + b2\n",
    "        tf.summary.histogram('output-pre-active', a)\n",
    "\n",
    "        output = tf.nn.relu(a)\n",
    "        tf.summary.histogram('output-activation', output)\n",
    "\n",
    "\n",
    "with tf.name_scope('loss'):\n",
    "    loss = tf.reduce_mean(tf.reduce_sum(tf.square(y-output),reduction_indices=[1]))\n",
    "    tf.summary.scalar('loss',loss)\n",
    "\n",
    "\n",
    "with tf.name_scope('train'):\n",
    "    train = tf.train.GradientDescentOptimizer(0.1).minimize(loss)\n",
    "\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "merge = tf.summary.merge_all()\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    sess.run(init)\n",
    "    writer = tf.summary.FileWriter(log_dir,sess.graph)\n",
    "\n",
    "    for i in range(500) :\n",
    "        sess.run(train,feed_dict={x:x_data,y:y_data})\n",
    "        result = sess.run(merge,feed_dict={x:x_data,y:y_data})\n",
    "        writer.add_summary(result,i)\n",
    "        \n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding projector in Tensorboard \n",
    "Demo from official tensorboard \n",
    "https://github.com/decentralion/tf-dev-summit-tensorboard-tutorial/blob/master/mnist.py \n",
    "[Demo for projector](https://projector.tensorflow.org/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-6-5291e38d75fa>:12: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From /home/nana/.local/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From /home/nana/.local/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:252: _internal_retry.<locals>.wrap.<locals>.wrapped_fn (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use urllib or similar directly.\n",
      "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
      "WARNING:tensorflow:From /home/nana/.local/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting ./logs_tensorboard_demo/2018-08-10-12-08-29/data/train-images-idx3-ubyte.gz\n",
      "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
      "WARNING:tensorflow:From /home/nana/.local/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting ./logs_tensorboard_demo/2018-08-10-12-08-29/data/train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /home/nana/.local/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
      "Extracting ./logs_tensorboard_demo/2018-08-10-12-08-29/data/t10k-images-idx3-ubyte.gz\n",
      "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
      "Extracting ./logs_tensorboard_demo/2018-08-10-12-08-29/data/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /home/nana/.local/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "Starting run for lr_1E-03,conv=1,fc=1\n",
      "WARNING:tensorflow:From <ipython-input-6-5291e38d75fa>:80: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See @{tf.nn.softmax_cross_entropy_with_logits_v2}.\n",
      "\n",
      "Done training!\n",
      "Run `tensorboard --logdir=./logs_tensorboard_demo/2018-08-10-12-08-29/` to see the results.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import os.path\n",
    "import tensorflow as tf\n",
    "import arrow\n",
    "LOGDIR=\"./logs_tensorboard_demo/{}/\".format(arrow.now().format('YYYY-MM-DD-HH-MM-SS'))\n",
    "\n",
    "# You need to download these two files from the github link and put in the current folder same as notebook \n",
    "LABELS = os.path.join(os.getcwd(), \"labels_1024.tsv\")\n",
    "SPRITES = os.path.join(os.getcwd(), \"sprite_1024.png\")\n",
    "\n",
    "### MNIST EMBEDDINGS ###\n",
    "mnist = tf.contrib.learn.datasets.mnist.read_data_sets(train_dir=LOGDIR + \"data\", one_hot=True)\n",
    "\n",
    "### Get a sprite and labels file for the embedding projector ###\n",
    "# if not (os.path.isfile(LABELS) and os.path.isfile(SPRITES)):\n",
    "#     print(\"Necessary data files were not found. Run this command from inside the \"\n",
    "#           \"repo provided at \"\n",
    "#           \"https://github.com/dandelionmane/tf-dev-summit-tensorboard-tutorial.\")\n",
    "#     os.exit(1)\n",
    "# shutil.copyfile(LABELS, os.path.join(LOGDIR, LABELS))\n",
    "# shutil.copyfile(SPRITES, os.path.join(LOGDIR, SPRITES))\n",
    "\n",
    "\n",
    "def conv_layer(input, size_in, size_out, name=\"conv\"):\n",
    "    with tf.name_scope(name):\n",
    "        w = tf.Variable(tf.truncated_normal([5, 5, size_in, size_out], stddev=0.1), name=\"W\")\n",
    "        b = tf.Variable(tf.constant(0.1, shape=[size_out]), name=\"B\")\n",
    "        conv = tf.nn.conv2d(input, w, strides=[1, 1, 1, 1], padding=\"SAME\")\n",
    "        act = tf.nn.relu(conv + b)\n",
    "        tf.summary.histogram(\"weights\", w)\n",
    "        tf.summary.histogram(\"biases\", b)\n",
    "        tf.summary.histogram(\"activations\", act)\n",
    "        return tf.nn.max_pool(act, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding=\"SAME\")\n",
    "\n",
    "\n",
    "def fc_layer(input, size_in, size_out, name=\"fc\"):\n",
    "    with tf.name_scope(name):\n",
    "        w = tf.Variable(tf.truncated_normal([size_in, size_out], stddev=0.1), name=\"W\")\n",
    "        b = tf.Variable(tf.constant(0.1, shape=[size_out]), name=\"B\")\n",
    "        act = tf.matmul(input, w) + b\n",
    "        tf.summary.histogram(\"weights\", w)\n",
    "        tf.summary.histogram(\"biases\", b)\n",
    "        tf.summary.histogram(\"activations\", act)\n",
    "        return act\n",
    "\n",
    "\n",
    "def mnist_model(learning_rate, use_two_fc, use_two_conv, hparam):\n",
    "    # reset and clean the default graph in stack\n",
    "    tf.reset_default_graph()\n",
    "    with tf.Session() as sess:\n",
    "        \n",
    "        # Setup placeholders, and reshape the data\n",
    "        x = tf.placeholder(tf.float32, shape=[None, 784], name=\"x\")\n",
    "        x_image = tf.reshape(x, [-1, 28, 28, 1])\n",
    "        tf.summary.image('input', x_image,max_outputs=10)\n",
    "        y = tf.placeholder(tf.float32, shape=[None, 10], name=\"labels\")\n",
    "\n",
    "        if use_two_conv:\n",
    "            conv1 = conv_layer(x_image, 1, 32, \"conv1\")\n",
    "            conv_out = conv_layer(conv1, 32, 64, \"conv2\")\n",
    "        else:\n",
    "            conv_out = conv_layer(x_image, 1, 16, \"conv\")\n",
    "\n",
    "        flattened = tf.reshape(conv_out, [-1, 7 * 7 * 64])\n",
    "\n",
    "        if use_two_fc:\n",
    "            fc1 = fc_layer(flattened, 7 * 7 * 64, 1024, \"fc1\")\n",
    "            relu = tf.nn.relu(fc1)\n",
    "            embedding_input = relu\n",
    "            tf.summary.histogram(\"fc1/relu\", relu)\n",
    "            embedding_size = 1024\n",
    "            logits = fc_layer(relu, 1024, 10, \"fc2\")\n",
    "        else:\n",
    "            embedding_input = flattened\n",
    "            embedding_size = 7 * 7 * 64\n",
    "            logits = fc_layer(flattened, 7 * 7 * 64, 10, \"fc\")\n",
    "\n",
    "        with tf.name_scope(\"loss\"):\n",
    "            xent = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
    "                    logits=logits, labels=y), name=\"loss\")\n",
    "            tf.summary.scalar(\"loss\", xent)\n",
    "\n",
    "        with tf.name_scope(\"train\"):\n",
    "            train_step = tf.train.AdamOptimizer(learning_rate).minimize(xent)\n",
    "\n",
    "        with tf.name_scope(\"accuracy\"):\n",
    "            correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(y, 1))\n",
    "            accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "            tf.summary.scalar(\"accuracy\", accuracy)\n",
    "\n",
    "        summ = tf.summary.merge_all()\n",
    "\n",
    "        embedding = tf.Variable(tf.zeros([1024, embedding_size]), name=\"test_embedding\")\n",
    "        assignment = embedding.assign(embedding_input)\n",
    "        \n",
    "        # The embedding projector will read the embeddings from your model checkpoint file\n",
    "        saver = tf.train.Saver()\n",
    "\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        writer = tf.summary.FileWriter(LOGDIR + hparam)\n",
    "        writer.add_graph(sess.graph)\n",
    "        \n",
    "        # the projector code\n",
    "        config = tf.contrib.tensorboard.plugins.projector.ProjectorConfig()\n",
    "        embedding_config = config.embeddings.add()\n",
    "        embedding_config.tensor_name = embedding.name\n",
    "        embedding_config.sprite.image_path = SPRITES\n",
    "        embedding_config.metadata_path = LABELS\n",
    "        # Specify the width and height of a single thumbnail.\n",
    "        embedding_config.sprite.single_image_dim.extend([28, 28])\n",
    "        tf.contrib.tensorboard.plugins.projector.visualize_embeddings(writer, config)\n",
    "\n",
    "        for i in range(2001):\n",
    "            batch = mnist.train.next_batch(100)\n",
    "            if i % 5 == 0:\n",
    "                [train_accuracy, s] = sess.run([accuracy, summ], feed_dict={x: batch[0], y: batch[1]})\n",
    "                writer.add_summary(s, i)\n",
    "            if i % 500 == 0:\n",
    "                sess.run(assignment, feed_dict={x: mnist.test.images[:1024], y: mnist.test.labels[:1024]})\n",
    "                saver.save(sess, os.path.join(LOGDIR, \"model.ckpt\"), i)\n",
    "            sess.run(train_step, feed_dict={x: batch[0], y: batch[1]})\n",
    "    \n",
    "\n",
    "def make_hparam_string(learning_rate, use_two_fc, use_two_conv):\n",
    "    conv_param = \"conv=2\" if use_two_conv else \"conv=1\"\n",
    "    fc_param = \"fc=2\" if use_two_fc else \"fc=1\"\n",
    "    return \"lr_%.0E,%s,%s\" % (learning_rate, conv_param, fc_param)\n",
    "\n",
    "\n",
    "def main():\n",
    "    # You can try adding some more learning rates\n",
    "    for learning_rate in [1E-3]: #, 1E-4\n",
    "        # Include \"False\" as a value to try different model architectures\n",
    "        for use_two_fc in [False]:\n",
    "            for use_two_conv in [False]: #, True\n",
    "                # Construct a hyperparameter string for each one (example: \"lr_1E-3,fc=2,conv=2\")\n",
    "                hparam = make_hparam_string(learning_rate, use_two_fc, use_two_conv)\n",
    "                print('Starting run for %s' % hparam)\n",
    "\n",
    "                # Actually run with the new settings\n",
    "                mnist_model(learning_rate, use_two_fc, use_two_conv, hparam)\n",
    "    print('Done training!')\n",
    "    print('Run `tensorboard --logdir=%s` to see the results.' % LOGDIR)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable                     Type                          Data/Info\n",
      "--------------------------------------------------------------------\n",
      "Adadelta                     type                          <class 'tensorflow.python<...>ras.optimizers.Adadelta'>\n",
      "Adagrad                      type                          <class 'tensorflow.python<...>eras.optimizers.Adagrad'>\n",
      "Adam                         type                          <class 'tensorflow.python<...>l.keras.optimizers.Adam'>\n",
      "Adamax                       type                          <class 'tensorflow.python<...>keras.optimizers.Adamax'>\n",
      "Dense                        type                          <class 'tensorflow.python<...>keras.layers.core.Dense'>\n",
      "GRU                          type                          <class 'tensorflow.python<...>as.layers.recurrent.GRU'>\n",
      "Input                        function                      <function Input at 0x7fa994d1d9d8>\n",
      "LABELS                       str                           /home/nana/Prometheus/src/labels_1024.tsv\n",
      "LOGDIR                       str                           ./logs_tensorboard_demo/2018-08-10-12-08-29/\n",
      "MinMaxScaler                 type                          <class 'sklearn.preprocessing.data.MinMaxScaler'>\n",
      "Nadam                        type                          <class 'tensorflow.python<...>.keras.optimizers.Nadam'>\n",
      "Optimizer                    type                          <class 'tensorflow.python<...>as.optimizers.Optimizer'>\n",
      "RMSprop                      type                          <class 'tensorflow.python<...>eras.optimizers.RMSprop'>\n",
      "SGD                          type                          <class 'tensorflow.python<...>pl.keras.optimizers.SGD'>\n",
      "SPRITES                      str                           /home/nana/Prometheus/src/sprite_1024.png\n",
      "Sequential                   type                          <class 'tensorflow.python<...>e.sequential.Sequential'>\n",
      "TensorBoard                  type                          <class 'tensorflow.python<...>s.callbacks.TensorBoard'>\n",
      "TensorBoard_ValidateScalar   type                          <class '__main__.TensorBoard_ValidateScalar'>\n",
      "W1                           Variable                      <tf.Variable 'hidden_laye<...>1, 10) dtype=float32_ref>\n",
      "W2                           Variable                      <tf.Variable 'output_laye<...>10, 1) dtype=float32_ref>\n",
      "X                            ndarray                       8710x77: 670670 elems, type `float64`, 5365360 bytes (5.1168060302734375 Mb)\n",
      "Y                            ndarray                       8710x24: 209040 elems, type `float64`, 1672320 bytes (1.5948486328125 Mb)\n",
      "a                            Tensor                        Tensor(\"output_layer/Acti<...>pe=(?, 1), dtype=float32)\n",
      "arrow                        module                        <module 'arrow' from '/us<...>kages/arrow/__init__.py'>\n",
      "b1                           Tensor                        Tensor(\"hidden_layer/B/Co<...>e=(1, 10), dtype=float32)\n",
      "b2                           Tensor                        Tensor(\"output_layer/B/Co<...>pe=(1, 1), dtype=float32)\n",
      "batch_generator              function                      <function batch_generator at 0x7faa0c20c6a8>\n",
      "calendar                     module                        <module 'calendar' from '<...>b/python3.6/calendar.py'>\n",
      "conv_layer                   function                      <function conv_layer at 0x7fa7df9139d8>\n",
      "deserialize                  function                      <function deserialize at 0x7fa994ce4ae8>\n",
      "df_X                         DataFrame                                          Utet<...>n[8760 rows x 77 columns]\n",
      "df_Y                         DataFrame                                             T<...>n[8760 rows x 24 columns]\n",
      "f                            TextIOWrapper                 <_io.TextIOWrapper name='<...>ode='w' encoding='UTF-8'>\n",
      "fc_layer                     function                      <function fc_layer at 0x7fa7df9138c8>\n",
      "get                          function                      <function get at 0x7fa994ce4b70>\n",
      "hidden                       Tensor                        Tensor(\"hidden_layer/Soft<...>e=(?, 10), dtype=float32)\n",
      "i                            int                           499\n",
      "init                         Operation                     name: \"init\"\\nop: \"NoOp\"\\<...>ayer/W/Variable/Assign\"\\n\n",
      "listdir                      builtin_function_or_method    <built-in function listdir>\n",
      "log_dir                      str                           ./logs_tensorboard_demo/2018-08-10-12-08-48/\n",
      "loss                         Tensor                        Tensor(\"loss/Mean:0\", shape=(), dtype=float32)\n",
      "main                         function                      <function main at 0x7fa7df929d90>\n",
      "make_hparam_string           function                      <function make_hparam_string at 0x7fa7df929e18>\n",
      "mean_squared_error           function                      <function mean_squared_error at 0x7fa994c55950>\n",
      "merge                        Tensor                        Tensor(\"Merge/MergeSummar<...>, shape=(), dtype=string)\n",
      "mnist                        Datasets                      Datasets(train=<tensorflo<...>bject at 0x7fa7df917390>)\n",
      "mnist_model                  function                      <function mnist_model at 0x7fa7df929ea0>\n",
      "model                        Sequential                    <tensorflow.python.keras.<...>object at 0x7fa935975550>\n",
      "noise                        ndarray                       300x1: 300 elems, type `float64`, 2400 bytes\n",
      "np                           module                        <module 'numpy' from '/us<...>kages/numpy/__init__.py'>\n",
      "num_train                    int                           6532\n",
      "num_x_signals                int                           77\n",
      "num_y_signals                int                           24\n",
      "os                           module                        <module 'os' from '/usr/lib/python3.6/os.py'>\n",
      "output                       Tensor                        Tensor(\"output_layer/Acti<...>pe=(?, 1), dtype=float32)\n",
      "pd                           module                        <module 'pandas' from '/h<...>ages/pandas/__init__.py'>\n",
      "pickle                       module                        <module 'pickle' from '/u<...>lib/python3.6/pickle.py'>\n",
      "plt                          module                        <module 'matplotlib.pyplo<...>es/matplotlib/pyplot.py'>\n",
      "ratio                        float                         0.75\n",
      "result                       bytes                         b'\\n\\xe4\\x02\\n\\x1chidden_<...>tloss/loss\\x15\\xe5[\\xe3='\n",
      "run_metadata                 RunMetadata                   step_stats {\\n  dev_stats<...>n      }\\n    }\\n  }\\n}\\n\n",
      "serialize                    function                      <function serialize at 0x7fa994ce0730>\n",
      "sess                         Session                       <tensorflow.python.client<...>object at 0x7fa935e59f28>\n",
      "shift_iter                   int                           24\n",
      "sns                          module                        <module 'seaborn' from '/<...>ges/seaborn/__init__.py'>\n",
      "tf                           module                        <module 'tensorflow' from<...>/tensorflow/__init__.py'>\n",
      "time                         module                        <module 'time' (built-in)>\n",
      "timeline                     module                        <module 'tensorflow.pytho<...>thon/client/timeline.py'>\n",
      "trace                        Timeline                      <tensorflow.python.client<...>object at 0x7fa934097ac8>\n",
      "train                        Operation                     name: \"train/GradientDesc<...>e/ApplyGradientDescent\"\\n\n",
      "validation_data              tuple                         n=2\n",
      "warnings                     module                        <module 'warnings' from '<...>b/python3.6/warnings.py'>\n",
      "writer                       FileWriter                    <tensorflow.python.summar<...>object at 0x7fa914503b70>\n",
      "x                            Tensor                        Tensor(\"input_layer/x_inp<...>pe=(?, 1), dtype=float32)\n",
      "x_data                       ndarray                       300x1: 300 elems, type `float64`, 2400 bytes\n",
      "x_scaler                     MinMaxScaler                  MinMaxScaler(copy=True, feature_range=(0, 1))\n",
      "x_test                       ndarray                       2178x77: 167706 elems, type `float64`, 1341648 bytes (1.2794952392578125 Mb)\n",
      "x_test_scaled                ndarray                       2178x77: 167706 elems, type `float64`, 1341648 bytes (1.2794952392578125 Mb)\n",
      "x_train                      ndarray                       6532x77: 502964 elems, type `float64`, 4023712 bytes (3.837310791015625 Mb)\n",
      "x_train_scaled               ndarray                       6532x77: 502964 elems, type `float64`, 4023712 bytes (3.837310791015625 Mb)\n",
      "y                            Tensor                        Tensor(\"input_layer/y_inp<...>pe=(?, 1), dtype=float32)\n",
      "y_data                       ndarray                       300x1: 300 elems, type `float64`, 2400 bytes\n",
      "y_scaler                     MinMaxScaler                  MinMaxScaler(copy=True, feature_range=(0, 1))\n",
      "y_test                       ndarray                       2178x24: 52272 elems, type `float64`, 418176 bytes (408.375 kb)\n",
      "y_test_scaled                ndarray                       2178x24: 52272 elems, type `float64`, 418176 bytes (408.375 kb)\n",
      "y_train                      ndarray                       6532x24: 156768 elems, type `float64`, 1254144 bytes (1.196044921875 Mb)\n",
      "y_train_scaled               ndarray                       6532x24: 156768 elems, type `float64`, 1254144 bytes (1.196044921875 Mb)\n"
     ]
    }
   ],
   "source": [
    "whos"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
