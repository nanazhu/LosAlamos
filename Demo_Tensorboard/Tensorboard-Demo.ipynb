{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tersor....Board....ing "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "#### Watching the [introduction video](https://www.youtube.com/watch?v=eBbEDRsCmv4&t=1105s), stop focusing on Dandelion Mané's fancy nail polishing  (that is the purple elephant with mint polka dot) [Demo code](https://github.com/decentralion/tf-dev-summit-tensorboard-tutorial)\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "If (Tensorboard == useful): \n",
    "    Install tensorflow https://www.tensorflow.org/install   \n",
    "else \n",
    "    Close this notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO \n",
    "    create the input data\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### start the tensorboard : \n",
    "\n",
    "    /Prometheus/src$ tensorboard --logdir=./logs_ann/\n",
    "....\n",
    "\n",
    "2018-08-02 07:44:08.927850: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 AVX512F FMA\n",
    "\n",
    "TensorBoard 1.8.0 at __http://*****:6006__ (Press CTRL+C to quit)\n",
    "\n",
    "\n",
    "Tips: if you open the 6006 url link but find nothing, calm down, double check the path. Wrong folder will give you empty board \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wrong code could too, so the code begins here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL']='2'\n",
    "\n",
    "from os import listdir\n",
    "from tensorflow.python.client import timeline\n",
    "\n",
    "import calendar\n",
    "import time\n",
    "import arrow\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.python.keras.models import Sequential\n",
    "from tensorflow.python.keras.layers import Dense, GRU,Input\n",
    "from tensorflow.python.keras.optimizers import *\n",
    "from tensorflow.python.keras.callbacks import TensorBoard\n",
    "from tensorflow.python.keras.losses import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8710, 77)\n",
      "(8710, 24)\n",
      "['Utetemperatur T+1', 'Energianvändning T+1', 'Utetemperatur T+2', 'Energianvändning T+2', 'Utetemperatur T+3', 'Energianvändning T+3', 'Utetemperatur T+4', 'Energianvändning T+4', 'Utetemperatur T+5', 'Energianvändning T+5', 'Utetemperatur T+6', 'Energianvändning T+6', 'Utetemperatur T+7', 'Energianvändning T+7', 'Utetemperatur T+8', 'Energianvändning T+8', 'Utetemperatur T+9', 'Energianvändning T+9', 'Utetemperatur T+10', 'Energianvändning T+10', 'Utetemperatur T+11', 'Energianvändning T+11', 'Utetemperatur T+12', 'Energianvändning T+12', 'Utetemperatur T+13', 'Energianvändning T+13', 'Utetemperatur T+14', 'Energianvändning T+14', 'Utetemperatur T+15', 'Energianvändning T+15', 'Utetemperatur T+16', 'Energianvändning T+16', 'Utetemperatur T+17', 'Energianvändning T+17', 'Utetemperatur T+18', 'Energianvändning T+18', 'Utetemperatur T+19', 'Energianvändning T+19', 'Utetemperatur T+20', 'Energianvändning T+20', 'Utetemperatur T+21', 'Energianvändning T+21', 'Utetemperatur T+22', 'Energianvändning T+22', 'Utetemperatur T+23', 'Energianvändning T+23', 'Utetemperatur T+24', 'Energianvändning T+24', 'hour', 'month', 'weekday', 'diff_hottest_hour', 'diff_coldest_day', 'Ta T-23', 'Ta T-22', 'Ta T-21', 'Ta T-20', 'Ta T-19', 'Ta T-18', 'Ta T-17', 'Ta T-16', 'Ta T-15', 'Ta T-14', 'Ta T-13', 'Ta T-12', 'Ta T-11', 'Ta T-10', 'Ta T-9', 'Ta T-8', 'Ta T-7', 'Ta T-6', 'Ta T-5', 'Ta T-4', 'Ta T-3', 'Ta T-2', 'Ta T-1', 'Ta T-0'] ['Ta T+1', 'Ta T+2', 'Ta T+3', 'Ta T+4', 'Ta T+5', 'Ta T+6', 'Ta T+7', 'Ta T+8', 'Ta T+9', 'Ta T+10', 'Ta T+11', 'Ta T+12', 'Ta T+13', 'Ta T+14', 'Ta T+15', 'Ta T+16', 'Ta T+17', 'Ta T+18', 'Ta T+19', 'Ta T+20', 'Ta T+21', 'Ta T+22', 'Ta T+23', 'Ta T+24']\n"
     ]
    }
   ],
   "source": [
    "# import data and tailor it for use\n",
    "import pickle\n",
    "with open(\"data\", \"rb\") as f:\n",
    "    df_X = pickle.load(f)\n",
    "    df_Y = pickle.load(f)\n",
    "\n",
    "shift_iter = 24\n",
    "X = df_X.values[shift_iter + 1:-shift_iter - 1]\n",
    "Y = df_Y.values[shift_iter + 1:-shift_iter - 1]\n",
    "print(X.shape)\n",
    "print(Y.shape)\n",
    "print(list(df_X),list(df_Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratio = 0.75  # for spliting train - test set\n",
    "num_train = round(X.shape[0] * ratio)\n",
    "x_train = X[:num_train]\n",
    "y_train = Y[:num_train]\n",
    "x_test = X[num_train:]\n",
    "y_test = Y[num_train:]\n",
    "\n",
    "x_scaler = MinMaxScaler()\n",
    "x_train_scaled = x_scaler.fit_transform(x_train)\n",
    "x_test_scaled = x_scaler.transform(x_test)\n",
    "\n",
    "y_scaler = MinMaxScaler()\n",
    "y_train_scaled = y_scaler.fit_transform(y_train)\n",
    "y_test_scaled = y_scaler.transform(y_test)\n",
    "\n",
    "validation_data = (np.expand_dims(x_test_scaled, axis=0),\n",
    "                   np.expand_dims(y_test_scaled, axis=0))\n",
    "\n",
    "num_x_signals = X.shape[1]\n",
    "num_y_signals = Y.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_generator(batch_size, sequence_length):\n",
    "    \"\"\"\n",
    "    Generator function for creating random batches of training-data.\n",
    "    \"\"\"\n",
    "\n",
    "    # Infinite loop.\n",
    "    while True:\n",
    "        # Allocate a new array for the batch of input-signals.\n",
    "        x_shape = (batch_size, sequence_length, num_x_signals)\n",
    "        x_batch = np.zeros(shape=x_shape, dtype=np.float16)\n",
    "\n",
    "        # Allocate a new array for the batch of output-signals.\n",
    "        y_shape = (batch_size, sequence_length, num_y_signals)\n",
    "        y_batch = np.zeros(shape=y_shape, dtype=np.float16)\n",
    "\n",
    "        # Fill the batch with random sequences of data.\n",
    "        for i in range(batch_size):\n",
    "            # Get a random start-index.\n",
    "            # This points somewhere into the training-data.\n",
    "            idx = np.random.randint(num_train - sequence_length)\n",
    "\n",
    "            # Copy the sequences of data starting at this index.\n",
    "            x_batch[i] = x_train_scaled[idx:idx + sequence_length]\n",
    "            y_batch[i] = y_train_scaled[idx:idx + sequence_length]\n",
    "\n",
    "        yield (x_batch, y_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### create a model from tensorflow using API : Keras "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TODO change the simple_value into histogram DONE\n",
    "\n",
    "class TrainValTensorBoard(TensorBoard):\n",
    "    def __init__(self, log_dir='./logs_for_tensorboard', **kwargs):\n",
    "        # Make the original `TensorBoard` log to a subdirectory 'training'\n",
    "        training_log_dir = os.path.join(log_dir, 'training')\n",
    "        super().__init__(training_log_dir, **kwargs)\n",
    "\n",
    "        # Log the validation metrics to a separate subdirectory\n",
    "        self.val_log_dir = os.path.join(log_dir, 'validation')\n",
    "\n",
    "    def set_model(self, model):\n",
    "        # Setup writer for validation metrics\n",
    "        self.val_writer = tf.summary.FileWriter(self.val_log_dir)\n",
    "        super().set_model(model)\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        # Pop the validation logs and handle them separately with\n",
    "        # `self.val_writer`. Also rename the keys so that they can\n",
    "        # be plotted on the same figure with the training metrics\n",
    "        logs = logs or {}\n",
    "        val_logs = {k.replace('val_', ''): v for k, v in logs.items() if k.startswith('val_')}\n",
    "        print(\"\\n\",\"val_logs\", \"\\n\",val_logs)\n",
    "        for name, value in val_logs.items():\n",
    "            \n",
    "#             # histogram in Tensorboard\n",
    "#             values = np.array(value)\n",
    "#             # Create histogram using numpy\n",
    "#             counts, bin_edges = np.histogram(values, bins=1000)\n",
    "#             # Fill fields of histogram proto\n",
    "#             hist = tf.HistogramProto()\n",
    "#             hist.min = float(np.min(values))\n",
    "#             hist.max = float(np.max(values))\n",
    "#             hist.num = int(np.prod(values.shape))\n",
    "#             hist.sum = float(np.sum(values))\n",
    "#             hist.sum_squares = float(np.sum(values ** 2))\n",
    "#             bin_edges = bin_edges[1:]\n",
    "\n",
    "#             # Add bin edges and counts\n",
    "#             for edge in bin_edges:\n",
    "#                 hist.bucket_limit.append(edge)\n",
    "#             for c in counts:\n",
    "#                 hist.bucket.append(c)\n",
    "#             summary = tf.Summary(value=[tf.Summary.Value(tag=name, histo=hist)])\n",
    "            \n",
    "#             self.val_writer.add_summary(summary, epoch)\n",
    "            # scalars in tensorboard\n",
    "            summary = tf.Summary(value=[tf.Summary.Value\n",
    "                              (tag=name, simple_value=value)\n",
    "                             ]\n",
    "                      )\n",
    "            \n",
    "            self.val_writer.add_summary(summary, epoch)\n",
    "        self.val_writer.flush()\n",
    "\n",
    "        # Pass the remaining logs to `TensorBoard.on_epoch_end`\n",
    "        logs = {k: v for k, v in logs.items() if not k.startswith('val_')}\n",
    "        super().on_epoch_end(epoch, logs)\n",
    "\n",
    "    def on_train_end(self, logs=None):\n",
    "        super().on_train_end(logs)\n",
    "        self.val_writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "hidden (Dense)               (None, None, 154)         12012     \n",
      "_________________________________________________________________\n",
      "output (Dense)               (None, None, 24)          3720      \n",
      "=================================================================\n",
      "Total params: 15,732\n",
      "Trainable params: 15,732\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(num_x_signals *2 ,\n",
    "              activation='hard_sigmoid',\n",
    "              name = \"hidden\",\n",
    "              input_shape=(None, num_x_signals,)))\n",
    "\n",
    "model.add(Dense(num_y_signals, \n",
    "                activation='linear',\n",
    "                name = \"output\"))\n",
    "\n",
    "model.compile(loss=mean_squared_error,\n",
    "              optimizer=Adam(lr=1e-2),\n",
    "              metrics=['accuracy','mae','mean_squared_error','mape'],\n",
    "              options = tf.RunOptions(trace_level = tf.RunOptions.FULL_TRACE),\n",
    "              run_metadata= tf.RunMetadata(), )\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./logs_tensorboard_demo/2018-08-02-14-08-60\n",
      "Epoch 1/10\n",
      "79/80 [============================>.] - ETA: 0s - loss: 0.0397 - acc: 0.0642 - mean_absolute_error: 0.1187 - mean_squared_error: 0.0397 - mean_absolute_percentage_error: 31112.6070\n",
      " val_logs \n",
      " {'loss': 0.002709698397666216, 'acc': 0.0555555559694767, 'mean_absolute_error': 0.04108614847064018, 'mean_squared_error': 0.002709698397666216, 'mean_absolute_percentage_error': 19.342060089111328}\n",
      "80/80 [==============================] - 5s 63ms/step - loss: 0.0393 - acc: 0.0644 - mean_absolute_error: 0.1178 - mean_squared_error: 0.0393 - mean_absolute_percentage_error: 30965.1173 - val_loss: 0.0027 - val_acc: 0.0556 - val_mean_absolute_error: 0.0411 - val_mean_squared_error: 0.0027 - val_mean_absolute_percentage_error: 19.3421\n",
      "Epoch 2/10\n",
      "79/80 [============================>.] - ETA: 0s - loss: 0.0025 - acc: 0.0913 - mean_absolute_error: 0.0387 - mean_squared_error: 0.0025 - mean_absolute_percentage_error: 15331.1771\n",
      " val_logs \n",
      " {'loss': 0.0021177511662244797, 'acc': 0.08448117226362228, 'mean_absolute_error': 0.03659852594137192, 'mean_squared_error': 0.0021177511662244797, 'mean_absolute_percentage_error': 17.582141876220703}\n",
      "80/80 [==============================] - 5s 61ms/step - loss: 0.0025 - acc: 0.0915 - mean_absolute_error: 0.0386 - mean_squared_error: 0.0025 - mean_absolute_percentage_error: 15392.2312 - val_loss: 0.0021 - val_acc: 0.0845 - val_mean_absolute_error: 0.0366 - val_mean_squared_error: 0.0021 - val_mean_absolute_percentage_error: 17.5821\n",
      "Epoch 3/10\n",
      "79/80 [============================>.] - ETA: 0s - loss: 0.0019 - acc: 0.1148 - mean_absolute_error: 0.0337 - mean_squared_error: 0.0019 - mean_absolute_percentage_error: 13917.4235\n",
      " val_logs \n",
      " {'loss': 0.0018360388930886984, 'acc': 0.08448117226362228, 'mean_absolute_error': 0.03397592157125473, 'mean_squared_error': 0.0018360388930886984, 'mean_absolute_percentage_error': 16.28940200805664}\n",
      "80/80 [==============================] - 5s 62ms/step - loss: 0.0019 - acc: 0.1150 - mean_absolute_error: 0.0337 - mean_squared_error: 0.0019 - mean_absolute_percentage_error: 13931.9617 - val_loss: 0.0018 - val_acc: 0.0845 - val_mean_absolute_error: 0.0340 - val_mean_squared_error: 0.0018 - val_mean_absolute_percentage_error: 16.2894\n",
      "Epoch 4/10\n",
      "79/80 [============================>.] - ETA: 0s - loss: 0.0015 - acc: 0.1350 - mean_absolute_error: 0.0307 - mean_squared_error: 0.0015 - mean_absolute_percentage_error: 11658.0359\n",
      " val_logs \n",
      " {'loss': 0.001679214183241129, 'acc': 0.09090909361839294, 'mean_absolute_error': 0.03230626508593559, 'mean_squared_error': 0.001679214183241129, 'mean_absolute_percentage_error': 15.487987518310547}\n",
      "80/80 [==============================] - 5s 62ms/step - loss: 0.0015 - acc: 0.1350 - mean_absolute_error: 0.0307 - mean_squared_error: 0.0015 - mean_absolute_percentage_error: 11791.3363 - val_loss: 0.0017 - val_acc: 0.0909 - val_mean_absolute_error: 0.0323 - val_mean_squared_error: 0.0017 - val_mean_absolute_percentage_error: 15.4880\n",
      "Epoch 5/10\n",
      "79/80 [============================>.] - ETA: 0s - loss: 0.0013 - acc: 0.1523 - mean_absolute_error: 0.0289 - mean_squared_error: 0.0013 - mean_absolute_percentage_error: 11381.4544\n",
      " val_logs \n",
      " {'loss': 0.001509721390902996, 'acc': 0.09320477396249771, 'mean_absolute_error': 0.030406475067138672, 'mean_squared_error': 0.001509721390902996, 'mean_absolute_percentage_error': 14.416203498840332}\n",
      "80/80 [==============================] - 5s 62ms/step - loss: 0.0013 - acc: 0.1524 - mean_absolute_error: 0.0289 - mean_squared_error: 0.0013 - mean_absolute_percentage_error: 11306.4931 - val_loss: 0.0015 - val_acc: 0.0932 - val_mean_absolute_error: 0.0304 - val_mean_squared_error: 0.0015 - val_mean_absolute_percentage_error: 14.4162\n",
      "Epoch 6/10\n",
      "79/80 [============================>.] - ETA: 0s - loss: 0.0013 - acc: 0.1651 - mean_absolute_error: 0.0279 - mean_squared_error: 0.0013 - mean_absolute_percentage_error: 11226.5422\n",
      " val_logs \n",
      " {'loss': 0.0014893338084220886, 'acc': 0.09044995158910751, 'mean_absolute_error': 0.030183222144842148, 'mean_squared_error': 0.0014893338084220886, 'mean_absolute_percentage_error': 14.420255661010742}\n",
      "80/80 [==============================] - 5s 62ms/step - loss: 0.0013 - acc: 0.1651 - mean_absolute_error: 0.0279 - mean_squared_error: 0.0013 - mean_absolute_percentage_error: 11175.9660 - val_loss: 0.0015 - val_acc: 0.0904 - val_mean_absolute_error: 0.0302 - val_mean_squared_error: 0.0015 - val_mean_absolute_percentage_error: 14.4203\n",
      "Epoch 7/10\n",
      "79/80 [============================>.] - ETA: 0s - loss: 0.0012 - acc: 0.1738 - mean_absolute_error: 0.0271 - mean_squared_error: 0.0012 - mean_absolute_percentage_error: 11525.0882\n",
      " val_logs \n",
      " {'loss': 0.0014544219011440873, 'acc': 0.09274563938379288, 'mean_absolute_error': 0.029786042869091034, 'mean_squared_error': 0.0014544219011440873, 'mean_absolute_percentage_error': 14.222633361816406}\n",
      "80/80 [==============================] - 5s 61ms/step - loss: 0.0012 - acc: 0.1738 - mean_absolute_error: 0.0271 - mean_squared_error: 0.0012 - mean_absolute_percentage_error: 11561.6638 - val_loss: 0.0015 - val_acc: 0.0927 - val_mean_absolute_error: 0.0298 - val_mean_squared_error: 0.0015 - val_mean_absolute_percentage_error: 14.2226\n",
      "Epoch 8/10\n",
      "79/80 [============================>.] - ETA: 0s - loss: 0.0011 - acc: 0.1768 - mean_absolute_error: 0.0265 - mean_squared_error: 0.0011 - mean_absolute_percentage_error: 10055.0452\n",
      " val_logs \n",
      " {'loss': 0.0015108393272385001, 'acc': 0.07759412378072739, 'mean_absolute_error': 0.03041142225265503, 'mean_squared_error': 0.0015108393272385001, 'mean_absolute_percentage_error': 14.683873176574707}\n",
      "80/80 [==============================] - 5s 61ms/step - loss: 0.0011 - acc: 0.1769 - mean_absolute_error: 0.0265 - mean_squared_error: 0.0011 - mean_absolute_percentage_error: 10054.1587 - val_loss: 0.0015 - val_acc: 0.0776 - val_mean_absolute_error: 0.0304 - val_mean_squared_error: 0.0015 - val_mean_absolute_percentage_error: 14.6839\n",
      "Epoch 9/10\n",
      "79/80 [============================>.] - ETA: 0s - loss: 0.0011 - acc: 0.1816 - mean_absolute_error: 0.0260 - mean_squared_error: 0.0011 - mean_absolute_percentage_error: 9522.4127\n",
      " val_logs \n",
      " {'loss': 0.0020497310906648636, 'acc': 0.06152433529496193, 'mean_absolute_error': 0.036031417548656464, 'mean_squared_error': 0.0020497310906648636, 'mean_absolute_percentage_error': 17.72812843322754}\n",
      "80/80 [==============================] - 5s 61ms/step - loss: 0.0011 - acc: 0.1818 - mean_absolute_error: 0.0260 - mean_squared_error: 0.0011 - mean_absolute_percentage_error: 9512.0573 - val_loss: 0.0020 - val_acc: 0.0615 - val_mean_absolute_error: 0.0360 - val_mean_squared_error: 0.0020 - val_mean_absolute_percentage_error: 17.7281\n",
      "Epoch 10/10\n",
      "79/80 [============================>.] - ETA: 0s - loss: 0.0011 - acc: 0.1842 - mean_absolute_error: 0.0254 - mean_squared_error: 0.0011 - mean_absolute_percentage_error: 9724.7328\n",
      " val_logs \n",
      " {'loss': 0.002307649701833725, 'acc': 0.06887052208185196, 'mean_absolute_error': 0.03835391625761986, 'mean_squared_error': 0.002307649701833725, 'mean_absolute_percentage_error': 18.851762771606445}\n",
      "80/80 [==============================] - 5s 61ms/step - loss: 0.0011 - acc: 0.1843 - mean_absolute_error: 0.0254 - mean_squared_error: 0.0011 - mean_absolute_percentage_error: 9646.4364 - val_loss: 0.0023 - val_acc: 0.0689 - val_mean_absolute_error: 0.0384 - val_mean_squared_error: 0.0023 - val_mean_absolute_percentage_error: 18.8518\n",
      "CPU times: user 5min 21s, sys: 3min 40s, total: 9min 2s\n",
      "Wall time: 50.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "generator = batch_generator(batch_size=200, sequence_length=168)\n",
    "log_dir=\"./logs_tensorboard_demo/{}\".format(arrow.now().format('YYYY-MM-DD-HH-MM-SS'))\n",
    "print(log_dir)\n",
    "\n",
    "model.fit_generator(generator=generator,\n",
    "                    epochs=10,\n",
    "                    verbose = 1,\n",
    "                    steps_per_epoch=80,\n",
    "                    validation_data=validation_data,\n",
    "                    callbacks=[TrainValTensorBoard(\n",
    "                           log_dir,\n",
    "                           histogram_freq = 1,\n",
    "                           write_graph = True, # visualize the graph in TensorBoard\n",
    "                           write_images = True, #  write model weights to visualize as image in TensorBoard\n",
    "                           write_grads = True, # visualize gradient histograms in TensorBoard\n",
    "                                  )])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "trace = timeline.Timeline(step_stats = run_metadata.step_stats)\n",
    "with open('timeline1.ctf.json','w') as f:\n",
    "    f.write(trace.generate_chrome_trace_format())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
